{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c793976f",
   "metadata": {},
   "source": [
    "# Destatis Data Ingestion\n",
    "Download und Vorverarbeitung der Datens√§tze f√ºr:\n",
    "- JVA (Strafvollzug)\n",
    "- Zensus (Bev√∂lkerung)\n",
    "- Justizurteile (Verurteilte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e54641",
   "metadata": {},
   "source": [
    "Das initiale Setup von Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e6fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting local Spark: Destatis_Ingest\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Pfad f√ºr lokale Imports erweitern\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from src.config_local import get_spark_session\n",
    "from pyspark.sql.functions import col, substring, trim, when, lit, sum as _sum\n",
    "\n",
    "spark = get_spark_session(\"Destatis_Ingest\")\n",
    "\n",
    "# API Config\n",
    "TOKEN = \"HIER_EINEN_TOKEN_EINF√úGEN\" # Registrieren auf hhttps://www-genesis.destatis.de/datenbank/online#modal=login,register f√ºr einen API-Token \n",
    "BASE_URL = 'https://www-genesis.destatis.de/genesisWS/rest/2020/'\n",
    "\n",
    "RAW_DIR = \"../data/raw/\"\n",
    "PROC_DIR = \"../data/processed/\"\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(PROC_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093615dd",
   "metadata": {},
   "source": [
    "Diese Hilfsfunktion dient zum erstellen der Tabellen aus den Datens√§tzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d086ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_table(table_code, filename, force_reload=True):\n",
    "    if os.path.exists(filename) and not force_reload:\n",
    "        print(f\"√úberspringe download, file exists: {filename}\")\n",
    "        return True\n",
    "\n",
    "    print(f\"Starte Download: {table_code} ...\")\n",
    "    url = f\"{BASE_URL}data/tablefile\"\n",
    "    payload = {\n",
    "        \"name\": table_code,\n",
    "        \"area\": \"all\",\n",
    "        \"compress\": \"false\",\n",
    "        \"transpose\": \"false\",\n",
    "        \"startyear\": \"1976\",\n",
    "        \"endyear\": \"2024\",\n",
    "        \"language\": \"de\",\n",
    "        \"format\": \"ffcsv\",\n",
    "        \"job\": \"false\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        r = requests.post(url, data=payload, \n",
    "                          headers={'Content-Type': 'application/x-www-form-urlencoded', 'username': TOKEN, 'password': \"\"})\n",
    "        \n",
    "        if r.status_code == 200:\n",
    "            # Pr√ºfe auf Zip-Header\n",
    "            if r.content[:2] == b'PK': \n",
    "                with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "                    with z.open(z.namelist()[0]) as zf, open(filename, 'wb') as f:\n",
    "                        f.write(zf.read())\n",
    "            else:\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(r.text)\n",
    "            print(f\"Download erfolgreich: {filename}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"API Fehler {r.status_code}: {r.text[:200]}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Download fehlgeschlagen: {e}\")\n",
    "        return False\n",
    "\n",
    "def clean_number(c):\n",
    "    # Destatis nutzt oft \"-\", \".\" oder \"/\" f√ºr fehlende Werte/Null\n",
    "    return when(trim(col(c)).isin(\".\", \"-\", \"\", \"/\"), 0).otherwise(trim(col(c))).cast(\"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb9f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Download: 12411-0002 ...\n",
      "Download erfolgreich: ../data/raw/population_raw.csv\n",
      "Verarbeite Bev√∂lkerungsdaten\n",
      "Gespeichert: ../data/processed/population_clean.parquet\n",
      "+----+-----------+---------+\n",
      "|jahr|      pop_D|    pop_A|\n",
      "+----+-----------+---------+\n",
      "|1990| 7.417087E7|5582357.0|\n",
      "|1977|5.7460519E7|3892226.0|\n",
      "|2003|7.5189851E7|7341820.0|\n",
      "+----+-----------+---------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# Bev√∂lkerung (Zensus) 12411-0002\n",
    "FILE_RAW = f\"{RAW_DIR}population_raw.csv\"\n",
    "FILE_OUT = f\"{PROC_DIR}population_clean.parquet\"\n",
    "\n",
    "download_table(\"12411-0002\", FILE_RAW)\n",
    "\n",
    "print(\"Verarbeite Bev√∂lkerungsdaten\")\n",
    "df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(FILE_RAW)\n",
    "\n",
    "# Filter auf relevante Merkmale und Pivotierung\n",
    "df_clean = df.withColumn(\"jahr\", substring(col(\"time\"), 1, 4).cast(\"int\")) \\\n",
    "    .filter(col(\"2_variable_attribute_label\").isin(\"Deutsche\", \"Ausl√§nder\")) \\\n",
    "    .withColumn(\"anzahl\", clean_number(\"value\")) \\\n",
    "    .groupBy(\"jahr\").pivot(\"2_variable_attribute_label\").sum(\"anzahl\") \\\n",
    "    .select(col(\"jahr\"), col(\"Deutsche\").alias(\"pop_D\"), col(\"Ausl√§nder\").alias(\"pop_A\"))\n",
    "\n",
    "df_clean.write.mode(\"overwrite\").parquet(FILE_OUT)\n",
    "print(f\"Gespeichert: {FILE_OUT}\")\n",
    "df_clean.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549bd106",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373c757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk bereits vorhanden: 1976-1990\n",
      "Chunk bereits vorhanden: 1991-2005\n",
      "Chunk bereits vorhanden: 2006-2015\n",
      "Chunk bereits vorhanden: 2016-2024\n",
      "Gespeichert: ../data/processed/justiz_clean.parquet\n",
      "+----+--------------------+-------------+----------+--------------------+-----------+\n",
      "|jahr|            straftat|nationalitaet|geschlecht|        alter_gruppe|verurteilte|\n",
      "+----+--------------------+-------------+----------+--------------------+-----------+\n",
      "|1982|Andere Straftaten...|    Ausl√§nder|  weiblich|25 bis unter 30 J...|       65.0|\n",
      "|1989|Straftaten ohne S...|     Deutsche|  weiblich|30 bis unter 40 J...|    16120.0|\n",
      "|1981|Straftaten ohne S...|     Deutsche|  weiblich|30 bis unter 40 J...|    14380.0|\n",
      "|1988|   Urkundenf√§lschung|     Deutsche| Insgesamt|   50 Jahre und mehr|      827.0|\n",
      "|1978|Straftaten ohne S...|     Deutsche|  weiblich|30 bis unter 40 J...|    15188.0|\n",
      "+----+--------------------+-------------+----------+--------------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Justiz (Chunked Download)\n",
    "# Tabelle: 24311-0002 (Verurteilte nach Straftaten & Merkmalen)\n",
    "import time\n",
    "import glob\n",
    "\n",
    "TABLE_CODE = \"24311-0002\"\n",
    "FILE_OUT = f\"{PROC_DIR}justiz_clean.parquet\"\n",
    "\n",
    "# API limitiert gro√üe Zeitr√§ume, daher in Chunks aufteilen\n",
    "YEAR_CHUNKS = [\n",
    "    (1976, 1990),\n",
    "    (1991, 2005),\n",
    "    (2006, 2015),\n",
    "    (2016, 2024)\n",
    "]\n",
    "\n",
    "def download_chunk(start, end):\n",
    "    filename = f\"{RAW_DIR}justiz_flat_{start}_{end}.csv\"\n",
    "    \n",
    "    # Check ob schon da\n",
    "    if os.path.exists(filename) and os.path.getsize(filename) > 100:\n",
    "        print(f\"Chunk bereits vorhanden: {start}-{end}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Lade Chunk: {start}-{end}\")\n",
    "    payload = {\n",
    "        \"name\": TABLE_CODE,\n",
    "        \"area\": \"all\",\n",
    "        \"compress\": \"false\",\n",
    "        \"transpose\": \"false\",\n",
    "        \"startyear\": str(start),\n",
    "        \"endyear\": str(end),\n",
    "        \"language\": \"de\",\n",
    "        \"format\": \"ffcsv\",\n",
    "        \"job\": \"false\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        r = requests.post(f\"{BASE_URL}data/tablefile\", data=payload, \n",
    "                          headers={'Content-Type': 'application/x-www-form-urlencoded', \n",
    "                                   'username': TOKEN, \n",
    "                                   'password': \"\"})\n",
    "        \n",
    "        if r.status_code == 200 and \"{\\\"Code\\\":\" not in r.text[:100]:\n",
    "            if r.content[:2] == b'PK':\n",
    "                with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "                    with z.open(z.namelist()[0]) as zf, open(filename, 'wb') as f:\n",
    "                        f.write(zf.read())\n",
    "            else:\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(r.text)\n",
    "            print(f\"   -> Gespeichert: {filename}\")\n",
    "        else:\n",
    "            print(f\"   -> API Fehler {r.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   -> Download fehlgeschlagen: {e}\")\n",
    "\n",
    "# Downloads durchf√ºhren\n",
    "for s, e in YEAR_CHUNKS:\n",
    "    download_chunk(s, e)\n",
    "    time.sleep(1)\n",
    "\n",
    "# Spark Verarbeitung - alle \"Chunks\" laden\n",
    "# Absolute Pfade n√∂tig, weil Spark relative Pfade anders aufl√∂st als Python\n",
    "try:\n",
    "    JUSTIZ_FILES = [os.path.abspath(f) for f in glob.glob(f\"{RAW_DIR}justiz_flat_*.csv\")]\n",
    "    df_raw = spark.read.option(\"header\", \"true\") \\\n",
    "                       .option(\"delimiter\", \";\") \\\n",
    "                       .option(\"inferSchema\", \"false\") \\\n",
    "                       .csv(JUSTIZ_FILES)\n",
    "\n",
    "    df_clean = df_raw.select(\n",
    "        col(\"time\").alias(\"jahr\"),\n",
    "        col(\"2_variable_attribute_label\").alias(\"straftat\"),\n",
    "        col(\"3_variable_attribute_label\").alias(\"nationalitaet\"),\n",
    "        col(\"4_variable_attribute_label\").alias(\"geschlecht\"),\n",
    "        col(\"5_variable_attribute_label\").alias(\"alter_gruppe\"),\n",
    "        clean_number(\"value\").alias(\"verurteilte\")\n",
    "    )\n",
    "    \n",
    "    df_final = df_clean.withColumn(\"jahr\", substring(col(\"jahr\"), 1, 4).cast(\"int\"))\n",
    "\n",
    "    df_final.write.mode(\"overwrite\").parquet(FILE_OUT)\n",
    "    print(f\"Gespeichert: {FILE_OUT}\")\n",
    "    df_final.show(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Verarbeiten der Justiz-Daten: {e}\")\n",
    "    try:\n",
    "        print(\"Vorhandene Spalten in CSV:\", df_raw.columns)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba4d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Download: 24321-0001 ...\n",
      "Download erfolgreich: ../data/raw/prison_raw.csv\n",
      "‚öôÔ∏è Verarbeite Gef√§ngnisdaten...\n",
      "üíæ Gespeichert: ../data/processed/prison_clean.parquet\n",
      "+----+-------------+--------+\n",
      "|jahr|nationalitaet|insassen|\n",
      "+----+-------------+--------+\n",
      "|2018|     Deutsche| 34690.0|\n",
      "|2022|     Deutsche| 27995.0|\n",
      "|2020|     Deutsche| 30420.0|\n",
      "+----+-------------+--------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# Strafvollzug (JVA) 24321-0001\n",
    "FILE_RAW = f\"{RAW_DIR}prison_raw.csv\"\n",
    "FILE_OUT = f\"{PROC_DIR}prison_clean.parquet\"\n",
    "\n",
    "download_table(\"24321-0001\", FILE_RAW)\n",
    "\n",
    "print(\"Verarbeite Gef√§ngnisdaten...\")\n",
    "df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(FILE_RAW)\n",
    "\n",
    "# Aggregation nach Nationalit√§t (Position 3 in Destatis Schema)\n",
    "# Filter auf Gesamtwerte um Doppelz√§hlungen zu vermeiden\n",
    "df_clean = df.withColumn(\"jahr\", substring(col(\"time\"), 1, 4).cast(\"int\")) \\\n",
    "    .filter(col(\"3_variable_attribute_label\").isin(\"Deutsche\", \"Ausl√§nder\")) \\\n",
    "    .filter(col(\"4_variable_attribute_label\") == \"Insgesamt\") \\\n",
    "    .filter(col(\"5_variable_attribute_label\") == \"Insgesamt\") \\\n",
    "    .withColumn(\"insassen_raw\", clean_number(\"value\")) \\\n",
    "    .groupBy(\"jahr\", col(\"3_variable_attribute_label\").alias(\"nationalitaet\")) \\\n",
    "    .agg(_sum(\"insassen_raw\").alias(\"insassen\"))\n",
    "\n",
    "df_clean.write.mode(\"overwrite\").parquet(FILE_OUT)\n",
    "print(f\"Gespeichert: {FILE_OUT}\")\n",
    "df_clean.show(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
