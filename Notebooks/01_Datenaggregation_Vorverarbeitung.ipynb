{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c793976f",
   "metadata": {},
   "source": [
    "# Destatis Data Ingestion\n",
    "Download und Vorverarbeitung der Datensätze für:\n",
    "- JVA (Strafvollzug)\n",
    "- Zensus (Bevölkerung)\n",
    "- Justizurteile (Verurteilte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e54641",
   "metadata": {},
   "source": [
    "Das initiale Setup von Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Pfad für lokale Imports erweitern\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from src.config_local import get_spark_session\n",
    "from pyspark.sql.functions import col, substring, trim, when, lit, sum as _sum\n",
    "\n",
    "spark = get_spark_session(\"Destatis_Ingest\")\n",
    "\n",
    "# API Config\n",
    "TOKEN = \"HIER_EINEN_TOKEN_EINFÜGEN\" # Registrieren auf hhttps://www-genesis.destatis.de/datenbank/online#modal=login,register für einen API-Token \n",
    "BASE_URL = 'https://www-genesis.destatis.de/genesisWS/rest/2020/'\n",
    "\n",
    "RAW_DIR = \"../data/raw/\"\n",
    "PROC_DIR = \"../data/processed/\"\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(PROC_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093615dd",
   "metadata": {},
   "source": [
    "Diese Hilfsfunktion dient zum erstellen der Tabellen aus den Datensätzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d086ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_table(table_code, filename, force_reload=True):\n",
    "    if os.path.exists(filename) and not force_reload:\n",
    "        print(f\"Überspringe download, file exists: {filename}\")\n",
    "        return True\n",
    "\n",
    "    print(f\"Starte Download: {table_code} ...\")\n",
    "    url = f\"{BASE_URL}data/tablefile\"\n",
    "    payload = {\n",
    "        \"name\": table_code,\n",
    "        \"area\": \"all\",\n",
    "        \"compress\": \"false\",\n",
    "        \"transpose\": \"false\",\n",
    "        \"startyear\": \"1976\",\n",
    "        \"endyear\": \"2024\",\n",
    "        \"language\": \"de\",\n",
    "        \"format\": \"ffcsv\",\n",
    "        \"job\": \"false\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        r = requests.post(url, data=payload, \n",
    "                          headers={'Content-Type': 'application/x-www-form-urlencoded', 'username': TOKEN, 'password': \"\"})\n",
    "        \n",
    "        if r.status_code == 200:\n",
    "            # Prüfe auf Zip-Header\n",
    "            if r.content[:2] == b'PK': \n",
    "                with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "                    with z.open(z.namelist()[0]) as zf, open(filename, 'wb') as f:\n",
    "                        f.write(zf.read())\n",
    "            else:\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(r.text)\n",
    "            print(f\"Download erfolgreich: {filename}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"API Fehler {r.status_code}: {r.text[:200]}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Download fehlgeschlagen: {e}\")\n",
    "        return False\n",
    "\n",
    "def clean_number(c):\n",
    "    # Destatis nutzt oft \"-\", \".\" oder \"/\" für fehlende Werte/Null\n",
    "    return when(trim(col(c)).isin(\".\", \"-\", \"\", \"/\"), 0).otherwise(trim(col(c))).cast(\"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb9f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bevölkerung (Zensus) 12411-0002\n",
    "FILE_RAW = f\"{RAW_DIR}population_raw.csv\"\n",
    "FILE_OUT = f\"{PROC_DIR}population_clean.parquet\"\n",
    "\n",
    "download_table(\"12411-0002\", FILE_RAW)\n",
    "\n",
    "print(\"Verarbeite Bevölkerungsdaten\")\n",
    "df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(FILE_RAW)\n",
    "\n",
    "# Filter auf relevante Merkmale und Pivotierung\n",
    "df_clean = df.withColumn(\"jahr\", substring(col(\"time\"), 1, 4).cast(\"int\")) \\\n",
    "    .filter(col(\"2_variable_attribute_label\").isin(\"Deutsche\", \"Ausländer\")) \\\n",
    "    .withColumn(\"anzahl\", clean_number(\"value\")) \\\n",
    "    .groupBy(\"jahr\").pivot(\"2_variable_attribute_label\").sum(\"anzahl\") \\\n",
    "    .select(col(\"jahr\"), col(\"Deutsche\").alias(\"pop_D\"), col(\"Ausländer\").alias(\"pop_A\"))\n",
    "\n",
    "df_clean.write.mode(\"overwrite\").parquet(FILE_OUT)\n",
    "print(f\"Gespeichert: {FILE_OUT}\")\n",
    "df_clean.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549bd106",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Justiz (Chunked Download)\n",
    "# Tabelle: 24311-0002 (Verurteilte nach Straftaten & Merkmalen)\n",
    "import time\n",
    "import glob\n",
    "\n",
    "TABLE_CODE = \"24311-0002\"\n",
    "FILE_OUT = f\"{PROC_DIR}justiz_clean.parquet\"\n",
    "\n",
    "# API limitiert große Zeiträume, daher in Chunks aufteilen\n",
    "YEAR_CHUNKS = [\n",
    "    (1976, 1990),\n",
    "    (1991, 2005),\n",
    "    (2006, 2015),\n",
    "    (2016, 2024)\n",
    "]\n",
    "\n",
    "def download_chunk(start, end):\n",
    "    filename = f\"{RAW_DIR}justiz_flat_{start}_{end}.csv\"\n",
    "    \n",
    "    # Check ob schon da\n",
    "    if os.path.exists(filename) and os.path.getsize(filename) > 100:\n",
    "        print(f\"Chunk bereits vorhanden: {start}-{end}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Lade Chunk: {start}-{end}\")\n",
    "    payload = {\n",
    "        \"name\": TABLE_CODE,\n",
    "        \"area\": \"all\",\n",
    "        \"compress\": \"false\",\n",
    "        \"transpose\": \"false\",\n",
    "        \"startyear\": str(start),\n",
    "        \"endyear\": str(end),\n",
    "        \"language\": \"de\",\n",
    "        \"format\": \"ffcsv\",\n",
    "        \"job\": \"false\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        r = requests.post(f\"{BASE_URL}data/tablefile\", data=payload, \n",
    "                          headers={'Content-Type': 'application/x-www-form-urlencoded', \n",
    "                                   'username': TOKEN, \n",
    "                                   'password': \"\"})\n",
    "        \n",
    "        if r.status_code == 200 and \"{\\\"Code\\\":\" not in r.text[:100]:\n",
    "            if r.content[:2] == b'PK':\n",
    "                with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "                    with z.open(z.namelist()[0]) as zf, open(filename, 'wb') as f:\n",
    "                        f.write(zf.read())\n",
    "            else:\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(r.text)\n",
    "            print(f\"   -> Gespeichert: {filename}\")\n",
    "        else:\n",
    "            print(f\"   -> API Fehler {r.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   -> Download fehlgeschlagen: {e}\")\n",
    "\n",
    "# Downloads durchführen\n",
    "for s, e in YEAR_CHUNKS:\n",
    "    download_chunk(s, e)\n",
    "    time.sleep(1)\n",
    "\n",
    "# Spark Verarbeitung - alle \"Chunks\" laden\n",
    "# Absolute Pfade nötig, weil Spark relative Pfade anders auflöst als Python\n",
    "try:\n",
    "    JUSTIZ_FILES = [os.path.abspath(f) for f in glob.glob(f\"{RAW_DIR}justiz_flat_*.csv\")]\n",
    "    df_raw = spark.read.option(\"header\", \"true\") \\\n",
    "                       .option(\"delimiter\", \";\") \\\n",
    "                       .option(\"inferSchema\", \"false\") \\\n",
    "                       .csv(JUSTIZ_FILES)\n",
    "\n",
    "    df_clean = df_raw.select(\n",
    "        col(\"time\").alias(\"jahr\"),\n",
    "        col(\"2_variable_attribute_label\").alias(\"straftat\"),\n",
    "        col(\"3_variable_attribute_label\").alias(\"nationalitaet\"),\n",
    "        col(\"4_variable_attribute_label\").alias(\"geschlecht\"),\n",
    "        col(\"5_variable_attribute_label\").alias(\"alter_gruppe\"),\n",
    "        clean_number(\"value\").alias(\"verurteilte\")\n",
    "    )\n",
    "    \n",
    "    df_final = df_clean.withColumn(\"jahr\", substring(col(\"jahr\"), 1, 4).cast(\"int\"))\n",
    "\n",
    "    df_final.write.mode(\"overwrite\").parquet(FILE_OUT)\n",
    "    print(f\"Gespeichert: {FILE_OUT}\")\n",
    "    df_final.show(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Verarbeiten der Justiz-Daten: {e}\")\n",
    "    try:\n",
    "        print(\"Vorhandene Spalten in CSV:\", df_raw.columns)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strafvollzug (JVA) 24321-0001\n",
    "FILE_RAW = f\"{RAW_DIR}prison_raw.csv\"\n",
    "FILE_OUT = f\"{PROC_DIR}prison_clean.parquet\"\n",
    "\n",
    "download_table(\"24321-0001\", FILE_RAW)\n",
    "\n",
    "print(\"Verarbeite Gefängnisdaten...\")\n",
    "df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(FILE_RAW)\n",
    "\n",
    "# Aggregation nach Nationalität (Position 3 in Destatis Schema)\n",
    "# Filter auf Gesamtwerte um Doppelzählungen zu vermeiden\n",
    "df_clean = df.withColumn(\"jahr\", substring(col(\"time\"), 1, 4).cast(\"int\")) \\\n",
    "    .filter(col(\"3_variable_attribute_label\").isin(\"Deutsche\", \"Ausländer\")) \\\n",
    "    .filter(col(\"4_variable_attribute_label\") == \"Insgesamt\") \\\n",
    "    .filter(col(\"5_variable_attribute_label\") == \"Insgesamt\") \\\n",
    "    .withColumn(\"insassen_raw\", clean_number(\"value\")) \\\n",
    "    .groupBy(\"jahr\", col(\"3_variable_attribute_label\").alias(\"nationalitaet\")) \\\n",
    "    .agg(_sum(\"insassen_raw\").alias(\"insassen\"))\n",
    "\n",
    "df_clean.write.mode(\"overwrite\").parquet(FILE_OUT)\n",
    "print(f\"Gespeichert: {FILE_OUT}\")\n",
    "df_clean.show(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
